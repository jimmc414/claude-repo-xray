[
  {
    "method": "_do_execute_action",
    "line": 1788,
    "complexity": 31,
    "flow": [
      "-> action == NextAction.GENERATE_HYPOTHESIS?",
      "  -> action == NextAction.DESIGN_EXPERIMENT?",
      "    -> untested?",
      "      -> self.enable_concurrent and self.async_llm_client and len(untested) > 1?",
      "        try:",
      "          -> evaluations?",
      "            * for eval_result in evaluations:",
      "              -> eval_result.get('recommendation') == 'proceed'?",
      "        ! except Exception",
      "        ! except Exception",
      "    -> action == NextAction.EXECUTE_EXPERIMENT?",
      "      -> experiment_queue?",
      "        -> self.enable_concurrent and self.parallel_executor and len(experiment_queue) > 1?",
      "      -> action == NextAction.ANALYZE_RESULT?",
      "        -> results?",
      "          -> self.enable_concurrent and self.async_llm_client and len(results) > 1?",
      "            try:",
      "              -> analyses?",
      "                * for analysis in analyses:",
      "                  -> result_id?",
      "            ! except Exception",
      "            ! except Exception",
      "        -> action == NextAction.REFINE_HYPOTHESIS?",
      "          -> tested?",
      "          {self._actions_this_iteration}",
      "          -> action == NextAction.CONVERGE?",
      "            -> action == NextAction.PAUSE?"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [
      "self._actions_this_iteration"
    ],
    "conditions": [
      "action == NextAction.GENERATE_HYPOTHESIS",
      "action == NextAction.DESIGN_EXPERIMENT",
      "untested",
      "self.enable_concurrent and self.async_llm_client and len(untested) > 1",
      "evaluations",
      "eval_result.get('recommendation') == 'proceed'",
      "action == NextAction.EXECUTE_EXPERIMENT",
      "experiment_queue",
      "self.enable_concurrent and self.parallel_executor and len(experiment_queue) > 1",
      "action == NextAction.ANALYZE_RESULT",
      "results",
      "self.enable_concurrent and self.async_llm_client and len(results) > 1",
      "analyses",
      "result_id",
      "action == NextAction.REFINE_HYPOTHESIS",
      "tested",
      "action == NextAction.CONVERGE",
      "action == NextAction.PAUSE"
    ],
    "signature": "async def _do_execute_action(self, action: NextAction)",
    "docstring": "Internal async method to execute action (wrapped by stage tracking).",
    "file": "kosmos/agents/research_director.py",
    "priority_score": 0.667
  },
  {
    "method": "decide_next_action",
    "line": 1603,
    "complexity": 23,
    "flow": [
      "try:",
      "  -> metrics.budget_enabled?",
      "  -> Return(NextAction.CONVERGE)",
      "! except BudgetExceededError",
      "! except ImportError",
      "-> self._check_runtime_exceeded(...)?",
      "  -> Return(NextAction.CONVERGE)",
      "-> not hasattr(self, '_actions_this_iterat...')?",
      "  {self._actions_this_iteration}",
      "-> self._actions_this_iteration > MAX_ACTIONS_PER_ITERATION?",
      "  -> Return(NextAction.CONVERGE)",
      "-> self._should_check_convergence(...)?",
      "  -> Return(NextAction.CONVERGE)",
      "-> current_state == WorkflowState.GENERATING_HYPOTHESES?",
      "  -> Return(NextAction.GENERATE_HYPOTHESIS)",
      "  -> current_state == WorkflowState.DESIGNING_EXPERIMENTS?",
      "    -> untested?",
      "      -> Return(NextAction.DESIGN_EXPERIMENT)",
      "      -> self.research_plan.experiment_queue?",
      "        -> Return(NextAction.EXECUTE_EXPERIMENT)",
      "        -> self.research_plan.results?",
      "          -> Return(NextAction.ANALYZE_RESULT)",
      "          -> Return(NextAction.CONVERGE)",
      "    -> current_state == WorkflowState.EXECUTING?",
      "      -> self.research_plan.experiment_queue?",
      "        -> Return(NextAction.EXECUTE_EXPERIMENT)",
      "        -> self.research_plan.results?",
      "          -> Return(NextAction.ANALYZE_RESULT)",
      "          -> Return(NextAction.REFINE_HYPOTHESIS)",
      "      -> current_state == WorkflowState.ANALYZING?",
      "        -> not self.research_plan.results?",
      "          -> self.research_plan.experiment_queue?",
      "            -> Return(NextAction.EXECUTE_EXPERIMENT)",
      "            -> Return(NextAction.REFINE_HYPOTHESIS)",
      "        -> Return(NextAction.ANALYZE_RESULT)",
      "        -> current_state == WorkflowState.REFINING?",
      "          -> self.research_plan.tested_hypotheses?",
      "            -> Return(NextAction.REFINE_HYPOTHESIS)",
      "            -> Return(NextAction.GENERATE_HYPOTHESIS)",
      "          -> current_state == WorkflowState.CONVERGED?",
      "            -> Return(NextAction.CONVERGE)",
      "            -> current_state == WorkflowState.ERROR?",
      "              -> Return(NextAction.ERROR_RECOVERY)",
      "              -> Return(NextAction.GENERATE_HYPOTHESIS)"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [
      "self._actions_this_iteration"
    ],
    "conditions": [
      "metrics.budget_enabled",
      "self._check_runtime_exceeded(...)",
      "not hasattr(self, '_actions_this_iterat...')",
      "self._actions_this_iteration > MAX_ACTIONS_PER_ITERATION",
      "self._should_check_convergence(...)",
      "current_state == WorkflowState.GENERATING_HYPOTHESES",
      "current_state == WorkflowState.DESIGNING_EXPERIMENTS",
      "untested",
      "self.research_plan.experiment_queue",
      "self.research_plan.results",
      "current_state == WorkflowState.EXECUTING",
      "self.research_plan.experiment_queue",
      "self.research_plan.results",
      "current_state == WorkflowState.ANALYZING",
      "not self.research_plan.results",
      "self.research_plan.experiment_queue",
      "current_state == WorkflowState.REFINING",
      "self.research_plan.tested_hypotheses",
      "current_state == WorkflowState.CONVERGED",
      "current_state == WorkflowState.ERROR"
    ],
    "signature": "def decide_next_action(self) -> NextAction",
    "docstring": "Decide what to do next based on current workflow state and research plan.",
    "file": "kosmos/agents/research_director.py",
    "priority_score": 0.667
  },
  {
    "method": "execute",
    "line": 148,
    "complexity": 23,
    "flow": [
      "-> language is None?",
      "  -> R_EXECUTOR_AVAILABLE and self.r_executor?",
      "-> language == 'r'?",
      "  -> Return(self._execute_r(code))",
      "* while attempt < ...:",
      "  try:",
      "    -> result.success?",
      "      -> current_code != code and attempt > 1?",
      "      -> Return(result)",
      "      -> retry_on_error and attempt < self.max_retries?",
      "        -> fixed_code and fixed_code != current_code?",
      "        -> current_code != code?",
      "        -> Return(result)",
      "    -> retry_on_error and attempt < self.max_retries?",
      "      -> fixed_code and fixed_code != current_code?",
      "      -> Return(ExecutionResult(...))",
      "  ! except Exception",
      "-> Return(ExecutionResult(...))"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "language is None",
      "R_EXECUTOR_AVAILABLE and self.r_executor",
      "language == 'r'",
      "result.success",
      "current_code != code and attempt > 1",
      "retry_on_error and attempt < self.max_retries",
      "fixed_code and fixed_code != current_code",
      "current_code != code",
      "retry_on_error and attempt < self.max_retries",
      "fixed_code and fixed_code != current_code"
    ],
    "signature": "def execute(self, code: str, local_vars: Optional[Dict[str, Any]]=None, retry_on_error: bool=False, llm_client: Optional[Any]=None, language: Optional[str]=None) -> ExecutionResult",
    "docstring": "Execute code and capture results with self-correcting retry (Issue #54).",
    "file": "kosmos/execution/executor.py",
    "priority_score": 0.452
  },
  {
    "method": "modify_code_for_retry",
    "line": 692,
    "complexity": 18,
    "flow": [
      "-> llm_client and attempt <= 2?",
      "  try:",
      "    -> fixed and fixed != original_code?",
      "      -> Return(fixed)",
      "  ! except Exception",
      "-> 'KeyError' in error_type?",
      "  -> Return(self._fix_key_error(original_code, error))",
      "  -> 'FileNotFoundError' in error_type?",
      "    -> Return(self._fix_file_not_found(original_code, error))",
      "    -> 'NameError' in error_type?",
      "      -> Return(self._fix_name_error(original_code, error))",
      "      -> 'TypeError' in error_type?",
      "        -> Return(self._fix_type_error(original_code, error))",
      "        -> 'IndexError' in error_type?",
      "          -> Return(self._fix_index_error(original_code, error))",
      "          -> 'AttributeError' in error_type?",
      "            -> Return(self._fix_attribute_error(original_code, error))",
      "            -> 'ValueError' in error_type?",
      "              -> Return(self._fix_value_error(original_code, error))",
      "              -> 'ZeroDivisionError' in error_type?",
      "                -> Return(self._fix_zero_division(original_code, error))",
      "                -> 'ImportError' in error_type or 'ModuleNotFoundError' in error_type?",
      "                  -> Return(self._fix_import_error(original_code, error))",
      "                  -> 'PermissionError' in error_type?",
      "                    -> Return(self._fix_permission_error(original_code, error))",
      "                    -> 'MemoryError' in error_type?",
      "                      -> Return(self._fix_memory_error(original_code, error))",
      "-> Return(None)"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "llm_client and attempt <= 2",
      "fixed and fixed != original_code",
      "'KeyError' in error_type",
      "'FileNotFoundError' in error_type",
      "'NameError' in error_type",
      "'TypeError' in error_type",
      "'IndexError' in error_type",
      "'AttributeError' in error_type",
      "'ValueError' in error_type",
      "'ZeroDivisionError' in error_type",
      "'ImportError' in error_type or 'ModuleNotFoundError' in error_type",
      "'PermissionError' in error_type",
      "'MemoryError' in error_type"
    ],
    "signature": "def modify_code_for_retry(self, original_code: str, error: str, error_type: str, traceback_str: str='', attempt: int=1, llm_client: Optional[Any]=None) -> Optional[str]",
    "docstring": "Modify code based on error for retry (Issue #54 - Enhanced).",
    "file": "kosmos/execution/executor.py",
    "priority_score": 0.452
  },
  {
    "method": "generate",
    "line": 206,
    "complexity": 35,
    "flow": [
      "try:",
      "  -> model_override?",
      "    -> self.enable_auto_model_selection and not self.is_cli_mode?",
      "      -> complexity_analysis['recommendation'] == 'haiku'?",
      "      -> 'haiku' in selected_model.lower(...)?",
      "        -> 'sonnet' in selected_model.lower(...)?",
      "  -> self.cache and not bypass_cache?",
      "    -> cached_response is not None?",
      "      -> Return(response_text)",
      "  -> hasattr(response, 'usage') and response.usage?",
      "    -> hasattr(..., 'input_tokens')?",
      "    -> hasattr(..., 'output_tokens')?",
      "  -> stop_reason == 'max_tokens'?",
      "  -> self.cache and not bypass_cache?",
      "    -> hasattr(response, 'usage') and response.usage?",
      "      -> hasattr(..., 'input_tokens') and hasattr(..., 'output_tokens')?",
      "  -> Return(text)",
      "! except Exception"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "model_override",
      "self.enable_auto_model_selection and not self.is_cli_mode",
      "complexity_analysis['recommendation'] == 'haiku'",
      "'haiku' in selected_model.lower(...)",
      "'sonnet' in selected_model.lower(...)",
      "self.cache and not bypass_cache",
      "cached_response is not None",
      "hasattr(response, 'usage') and response.usage",
      "hasattr(..., 'input_tokens')",
      "hasattr(..., 'output_tokens')",
      "stop_reason == 'max_tokens'",
      "self.cache and not bypass_cache",
      "hasattr(response, 'usage') and response.usage",
      "hasattr(..., 'input_tokens') and hasattr(..., 'output_tokens')"
    ],
    "signature": "def generate(self, prompt: str, system: Optional[str]=None, max_tokens: Optional[int]=None, temperature: Optional[float]=None, stop_sequences: Optional[List[str]]=None, bypass_cache: bool=False, model_override: Optional[str]=None) -> str",
    "docstring": "Generate text from Claude.",
    "file": "kosmos/core/llm.py",
    "priority_score": 0.425
  },
  {
    "method": "get_client",
    "line": 612,
    "complexity": 9,
    "flow": [
      "-> _default_client is not None and not reset?",
      "  -> Return(_default_client)",
      "-> _default_client is not None and not reset?",
      "  -> Return(_default_client)",
      "-> _default_client is None or reset?",
      "  -> use_provider_system?",
      "    try:",
      "    ! except Exception",
      "-> Return(_default_client)"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "_default_client is not None and not reset",
      "_default_client is not None and not reset",
      "_default_client is None or reset",
      "use_provider_system"
    ],
    "signature": "def get_client(reset: bool=False, use_provider_system: bool=True) -> Union[ClaudeClient, LLMProvider]",
    "docstring": "Get or create default LLM client singleton.",
    "file": "kosmos/core/llm.py",
    "priority_score": 0.425
  },
  {
    "method": "_inject_attributes_to_nodes",
    "line": 116,
    "complexity": 42,
    "flow": [
      "* while parent:",
      "  -> parent.nodeType == parent.ELEMENT_NODE and parent.tagName == 'w:del'?",
      "    -> Return(True)",
      "-> Return(False)",
      "-> not elem.hasAttribute('w:rsidR')?",
      "-> not elem.hasAttribute('w:rsidRDefault')?",
      "-> not elem.hasAttribute('w:rsidP')?",
      "-> not elem.hasAttribute('w14:paraId')?",
      "-> not elem.hasAttribute('w14:textId')?",
      "-> is_inside_deletion(elem)?",
      "  -> not elem.hasAttribute('w:rsidDel')?",
      "  -> not elem.hasAttribute('w:rsidR')?",
      "-> not elem.hasAttribute('w:id')?",
      "-> not elem.hasAttribute('w:author')?",
      "-> not elem.hasAttribute('w:date')?",
      "-> elem.tagName in ('w:ins', 'w:del') and not elem.hasAttribute('w16du:dateUtc')?",
      "-> not elem.hasAttribute('w:author')?",
      "-> not elem.hasAttribute('w:date')?",
      "-> not elem.hasAttribute('w:initials')?",
      "-> not elem.hasAttribute('w16cex:dateUtc')?",
      "-> elem.firstChild and elem.firstChild.nodeType == elem.firstChild.TEXT_NODE?",
      "  -> text and text[0].isspace(...) or text[...].isspace(...)?",
      "    -> not elem.hasAttribute('xml:space')?",
      "* for node in nodes:",
      "  -> node.nodeType != node.ELEMENT_NODE?",
      "  -> node.tagName == 'w:p'?",
      "    -> node.tagName == 'w:r'?",
      "      -> node.tagName == 'w:t'?",
      "        -> node.tagName in ('w:ins', 'w:del')?",
      "          -> node.tagName == 'w:comment'?",
      "            -> node.tagName == 'w16cex:commentExtensible'?",
      "  * for elem in node.getElementsByTagName('w:p'):",
      "  * for elem in node.getElementsByTagName('w:r'):",
      "  * for elem in node.getElementsByTagName('w:t'):",
      "  * for tag in ('w:ins', 'w:del'):",
      "    * for elem in node.getElementsByTagName(tag):",
      "  * for elem in node.getElementsByTagName('w:comment'):",
      "  * for elem in node.getElementsByTagName('w16cex:commentExtens...'):"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "parent.nodeType == parent.ELEMENT_NODE and parent.tagName == 'w:del'",
      "not elem.hasAttribute('w:rsidR')",
      "not elem.hasAttribute('w:rsidRDefault')",
      "not elem.hasAttribute('w:rsidP')",
      "not elem.hasAttribute('w14:paraId')",
      "not elem.hasAttribute('w14:textId')",
      "is_inside_deletion(elem)",
      "not elem.hasAttribute('w:rsidDel')",
      "not elem.hasAttribute('w:rsidR')",
      "not elem.hasAttribute('w:id')",
      "not elem.hasAttribute('w:author')",
      "not elem.hasAttribute('w:date')",
      "elem.tagName in ('w:ins', 'w:del') and not elem.hasAttribute('w16du:dateUtc')",
      "not elem.hasAttribute('w:author')",
      "not elem.hasAttribute('w:date')",
      "not elem.hasAttribute('w:initials')",
      "not elem.hasAttribute('w16cex:dateUtc')",
      "elem.firstChild and elem.firstChild.nodeType == elem.firstChild.TEXT_NODE",
      "text and text[0].isspace(...) or text[...].isspace(...)",
      "not elem.hasAttribute('xml:space')",
      "node.nodeType != node.ELEMENT_NODE",
      "node.tagName == 'w:p'",
      "node.tagName == 'w:r'",
      "node.tagName == 'w:t'",
      "node.tagName in ('w:ins', 'w:del')",
      "node.tagName == 'w:comment'",
      "node.tagName == 'w16cex:commentExtensible'"
    ],
    "signature": "def _inject_attributes_to_nodes(self, nodes)",
    "docstring": "Inject RSID, author, and date attributes into DOM nodes where applicable.",
    "file": "kosmos-claude-scientific-skills/scientific-skills/document-skills/docx/scripts/document.py",
    "priority_score": 0.416
  },
  {
    "method": "suggest_deletion",
    "line": 482,
    "complexity": 22,
    "flow": [
      "-> elem.nodeName == 'w:r'?",
      "  -> elem.getElementsByTagName('w:delText')?",
      "  * for t_elem in list(...):",
      "    * while t_elem.firstChild:",
      "    * for i in range(...):",
      "  -> elem.hasAttribute('w:rsidR')?",
      "    -> not elem.hasAttribute('w:rsidDel')?",
      "  -> Return(del_wrapper)",
      "  -> elem.nodeName == 'w:p'?",
      "    -> elem.getElementsByTagName('w:ins') or elem.getElementsByTagName('w:del')?",
      "    -> is_numbered?",
      "      -> not rPr_list?",
      "    * for t_elem in list(...):",
      "      * while t_elem.firstChild:",
      "      * for i in range(...):",
      "    * for run in elem.getElementsByTagName('w:r'):",
      "      -> run.hasAttribute('w:rsidR')?",
      "        -> not run.hasAttribute('w:rsidDel')?",
      "    * for child in ...:",
      "    -> Return(elem)"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "elem.nodeName == 'w:r'",
      "elem.getElementsByTagName('w:delText')",
      "elem.hasAttribute('w:rsidR')",
      "not elem.hasAttribute('w:rsidDel')",
      "elem.nodeName == 'w:p'",
      "elem.getElementsByTagName('w:ins') or elem.getElementsByTagName('w:del')",
      "is_numbered",
      "not rPr_list",
      "run.hasAttribute('w:rsidR')",
      "not run.hasAttribute('w:rsidDel')"
    ],
    "signature": "def suggest_deletion(self, elem)",
    "docstring": "Mark a w:r or w:p element as deleted with tracked changes (in-place DOM manipulation).",
    "file": "kosmos-claude-scientific-skills/scientific-skills/document-skills/docx/scripts/document.py",
    "priority_score": 0.416
  },
  {
    "method": "analyze_corpus",
    "line": 658,
    "complexity": 13,
    "flow": [
      "-> not papers?",
      "  -> Return(analysis)",
      "* for paper in papers:",
      "  -> paper.year?",
      "-> self.extract_concepts?",
      "  * for paper in papers[...]:",
      "    try:",
      "      * for concept in extraction.concepts:",
      "    ! except Exception",
      "-> generate_insights?",
      "  try:",
      "    [DB: analysis.update(insights)]",
      "  ! except Exception",
      "-> Return(analysis)"
    ],
    "side_effects": [
      "DB: analysis.update(insights)"
    ],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "not papers",
      "paper.year",
      "self.extract_concepts",
      "generate_insights"
    ],
    "signature": "def analyze_corpus(self, papers: List[PaperMetadata], generate_insights: bool=True) -> Dict[str, Any]",
    "docstring": "Analyze a corpus of papers collectively.",
    "file": "kosmos/agents/literature_analyzer.py",
    "priority_score": 0.397
  },
  {
    "method": "find_related_papers",
    "line": 574,
    "complexity": 12,
    "flow": [
      "-> self.use_semantic_similarity?",
      "  try:",
      "    * for result in similar:",
      "      -> result['score'] >= similarity_threshold?",
      "  ! except Exception",
      "-> self.use_knowledge_graph and self.knowledge_graph?",
      "  try:",
      "    * for item in graph_related:",
      "  ! except Exception",
      "* for item in related:",
      "  -> paper_id not in seen_ids?",
      "-> Return(unique_related[...])"
    ],
    "side_effects": [],
    "inputs": [],
    "state_mutations": [],
    "conditions": [
      "self.use_semantic_similarity",
      "result['score'] >= similarity_threshold",
      "self.use_knowledge_graph and self.knowledge_graph",
      "paper_id not in seen_ids"
    ],
    "signature": "def find_related_papers(self, paper: PaperMetadata, max_results: int=10, similarity_threshold: float=0.7) -> List[Dict[str, Any]]",
    "docstring": "Find papers related through multiple pathways.",
    "file": "kosmos/agents/literature_analyzer.py",
    "priority_score": 0.397
  }
]